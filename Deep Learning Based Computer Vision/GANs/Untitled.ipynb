{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "# Importing the libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64 # We set the size of the batch.\n",
    "imageSize = 64 # We set the size of the generated images (64x64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\torchEnv\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/782] Loss_D: 1.7074 Loss_G: 6.1108\n",
      "[0/25][1/782] Loss_D: 1.0308 Loss_G: 5.0058\n",
      "[0/25][2/782] Loss_D: 1.1606 Loss_G: 6.2429\n",
      "[0/25][3/782] Loss_D: 0.8380 Loss_G: 6.1915\n",
      "[0/25][4/782] Loss_D: 0.7631 Loss_G: 6.0950\n",
      "[0/25][5/782] Loss_D: 0.9401 Loss_G: 7.4754\n",
      "[0/25][6/782] Loss_D: 0.7324 Loss_G: 7.4171\n",
      "[0/25][7/782] Loss_D: 0.8342 Loss_G: 8.2096\n",
      "[0/25][8/782] Loss_D: 0.9868 Loss_G: 8.0933\n",
      "[0/25][9/782] Loss_D: 0.7768 Loss_G: 10.0024\n",
      "[0/25][10/782] Loss_D: 0.4764 Loss_G: 7.7819\n",
      "[0/25][11/782] Loss_D: 0.5769 Loss_G: 11.4204\n",
      "[0/25][12/782] Loss_D: 0.1907 Loss_G: 9.5035\n",
      "[0/25][13/782] Loss_D: 0.5956 Loss_G: 8.5178\n",
      "[0/25][14/782] Loss_D: 1.0823 Loss_G: 15.3645\n",
      "[0/25][15/782] Loss_D: 0.5958 Loss_G: 12.5578\n",
      "[0/25][16/782] Loss_D: 0.4848 Loss_G: 6.2974\n",
      "[0/25][17/782] Loss_D: 3.7559 Loss_G: 16.2150\n",
      "[0/25][18/782] Loss_D: 0.4636 Loss_G: 16.2617\n",
      "[0/25][19/782] Loss_D: 0.5944 Loss_G: 9.3837\n",
      "[0/25][20/782] Loss_D: 1.7930 Loss_G: 15.0584\n",
      "[0/25][21/782] Loss_D: 0.2055 Loss_G: 13.9449\n",
      "[0/25][22/782] Loss_D: 0.4093 Loss_G: 7.4387\n",
      "[0/25][23/782] Loss_D: 2.5817 Loss_G: 17.6634\n",
      "[0/25][24/782] Loss_D: 0.1659 Loss_G: 19.1467\n",
      "[0/25][25/782] Loss_D: 0.3417 Loss_G: 14.4337\n",
      "[0/25][26/782] Loss_D: 0.2389 Loss_G: 5.7819\n",
      "[0/25][27/782] Loss_D: 4.1073 Loss_G: 18.4349\n",
      "[0/25][28/782] Loss_D: 0.1952 Loss_G: 20.6263\n",
      "[0/25][29/782] Loss_D: 0.4452 Loss_G: 16.4994\n",
      "[0/25][30/782] Loss_D: 0.1773 Loss_G: 8.3618\n",
      "[0/25][31/782] Loss_D: 1.0743 Loss_G: 14.9772\n",
      "[0/25][32/782] Loss_D: 0.1943 Loss_G: 15.0773\n",
      "[0/25][33/782] Loss_D: 0.1694 Loss_G: 10.5675\n",
      "[0/25][34/782] Loss_D: 0.3014 Loss_G: 6.3398\n",
      "[0/25][35/782] Loss_D: 1.4059 Loss_G: 20.4016\n",
      "[0/25][36/782] Loss_D: 0.2599 Loss_G: 22.5547\n",
      "[0/25][37/782] Loss_D: 0.2454 Loss_G: 19.3953\n",
      "[0/25][38/782] Loss_D: 0.0535 Loss_G: 13.2992\n",
      "[0/25][39/782] Loss_D: 0.1238 Loss_G: 5.9317\n",
      "[0/25][40/782] Loss_D: 1.4384 Loss_G: 18.8361\n",
      "[0/25][41/782] Loss_D: 0.3338 Loss_G: 21.2579\n",
      "[0/25][42/782] Loss_D: 0.1646 Loss_G: 18.5060\n",
      "[0/25][43/782] Loss_D: 0.2059 Loss_G: 12.5475\n",
      "[0/25][44/782] Loss_D: 0.0808 Loss_G: 5.3466\n",
      "[0/25][45/782] Loss_D: 1.4685 Loss_G: 17.8300\n",
      "[0/25][46/782] Loss_D: 0.1954 Loss_G: 21.5074\n",
      "[0/25][47/782] Loss_D: 0.1409 Loss_G: 19.9530\n",
      "[0/25][48/782] Loss_D: 0.1345 Loss_G: 14.6869\n",
      "[0/25][49/782] Loss_D: 0.1595 Loss_G: 7.7633\n",
      "[0/25][50/782] Loss_D: 0.7897 Loss_G: 15.2644\n",
      "[0/25][51/782] Loss_D: 0.0317 Loss_G: 16.2592\n",
      "[0/25][52/782] Loss_D: 0.2684 Loss_G: 12.7930\n",
      "[0/25][53/782] Loss_D: 0.1126 Loss_G: 6.2191\n",
      "[0/25][54/782] Loss_D: 1.7510 Loss_G: 22.7253\n",
      "[0/25][55/782] Loss_D: 0.3237 Loss_G: 25.4257\n",
      "[0/25][56/782] Loss_D: 0.5381 Loss_G: 23.7687\n",
      "[0/25][57/782] Loss_D: 0.1140 Loss_G: 18.1929\n",
      "[0/25][58/782] Loss_D: 0.0206 Loss_G: 9.9316\n",
      "[0/25][59/782] Loss_D: 0.9093 Loss_G: 19.0622\n",
      "[0/25][60/782] Loss_D: 0.3994 Loss_G: 19.3569\n",
      "[0/25][61/782] Loss_D: 0.1890 Loss_G: 15.4558\n",
      "[0/25][62/782] Loss_D: 0.0454 Loss_G: 9.7669\n",
      "[0/25][63/782] Loss_D: 0.1279 Loss_G: 6.2302\n",
      "[0/25][64/782] Loss_D: 0.9956 Loss_G: 20.8923\n",
      "[0/25][65/782] Loss_D: 1.1998 Loss_G: 20.9074\n",
      "[0/25][66/782] Loss_D: 0.1703 Loss_G: 17.8891\n",
      "[0/25][67/782] Loss_D: 0.0874 Loss_G: 11.9098\n",
      "[0/25][68/782] Loss_D: 0.0696 Loss_G: 5.2272\n",
      "[0/25][69/782] Loss_D: 1.5104 Loss_G: 21.4000\n",
      "[0/25][70/782] Loss_D: 0.4185 Loss_G: 24.3057\n",
      "[0/25][71/782] Loss_D: 0.7246 Loss_G: 21.9550\n",
      "[0/25][72/782] Loss_D: 0.2176 Loss_G: 17.3375\n",
      "[0/25][73/782] Loss_D: 0.2075 Loss_G: 11.9882\n",
      "[0/25][74/782] Loss_D: 0.0341 Loss_G: 5.3395\n",
      "[0/25][75/782] Loss_D: 1.2416 Loss_G: 13.9546\n",
      "[0/25][76/782] Loss_D: 0.1480 Loss_G: 16.9000\n",
      "[0/25][77/782] Loss_D: 0.3358 Loss_G: 12.4914\n",
      "[0/25][78/782] Loss_D: 0.2009 Loss_G: 7.9997\n",
      "[0/25][79/782] Loss_D: 0.1956 Loss_G: 5.0333\n",
      "[0/25][80/782] Loss_D: 0.7996 Loss_G: 9.3688\n",
      "[0/25][81/782] Loss_D: 0.4985 Loss_G: 8.7094\n",
      "[0/25][82/782] Loss_D: 0.1289 Loss_G: 5.2213\n",
      "[0/25][83/782] Loss_D: 0.3821 Loss_G: 6.5924\n",
      "[0/25][84/782] Loss_D: 0.2059 Loss_G: 7.1833\n",
      "[0/25][85/782] Loss_D: 0.3685 Loss_G: 5.7581\n",
      "[0/25][86/782] Loss_D: 0.3059 Loss_G: 5.0158\n",
      "[0/25][87/782] Loss_D: 0.9553 Loss_G: 10.8033\n",
      "[0/25][88/782] Loss_D: 0.9181 Loss_G: 8.0956\n",
      "[0/25][89/782] Loss_D: 0.1092 Loss_G: 5.2707\n",
      "[0/25][90/782] Loss_D: 0.4780 Loss_G: 6.1588\n",
      "[0/25][91/782] Loss_D: 0.2897 Loss_G: 6.1356\n",
      "[0/25][92/782] Loss_D: 0.3397 Loss_G: 5.9940\n",
      "[0/25][93/782] Loss_D: 0.1737 Loss_G: 6.0578\n",
      "[0/25][94/782] Loss_D: 0.5036 Loss_G: 6.9252\n",
      "[0/25][95/782] Loss_D: 0.5065 Loss_G: 3.5707\n",
      "[0/25][96/782] Loss_D: 0.8178 Loss_G: 11.6060\n",
      "[0/25][97/782] Loss_D: 1.3399 Loss_G: 7.2346\n",
      "[0/25][98/782] Loss_D: 0.1552 Loss_G: 4.8230\n",
      "[0/25][99/782] Loss_D: 0.7737 Loss_G: 9.5393\n",
      "[0/25][100/782] Loss_D: 1.7361 Loss_G: 4.8287\n",
      "[0/25][101/782] Loss_D: 0.3181 Loss_G: 4.9223\n",
      "[0/25][102/782] Loss_D: 0.3501 Loss_G: 6.5069\n",
      "[0/25][103/782] Loss_D: 0.3409 Loss_G: 4.2690\n",
      "[0/25][104/782] Loss_D: 0.2748 Loss_G: 5.5043\n",
      "[0/25][105/782] Loss_D: 0.3633 Loss_G: 4.3773\n",
      "[0/25][106/782] Loss_D: 0.4336 Loss_G: 4.3621\n",
      "[0/25][107/782] Loss_D: 0.5216 Loss_G: 5.0193\n",
      "[0/25][108/782] Loss_D: 0.3179 Loss_G: 5.3700\n",
      "[0/25][109/782] Loss_D: 0.2426 Loss_G: 4.7400\n",
      "[0/25][110/782] Loss_D: 0.5699 Loss_G: 4.8870\n",
      "[0/25][111/782] Loss_D: 0.6192 Loss_G: 5.6455\n",
      "[0/25][112/782] Loss_D: 0.2487 Loss_G: 5.1964\n",
      "[0/25][113/782] Loss_D: 0.3718 Loss_G: 3.6752\n",
      "[0/25][114/782] Loss_D: 0.7373 Loss_G: 9.1365\n",
      "[0/25][115/782] Loss_D: 2.0185 Loss_G: 2.2028\n",
      "[0/25][116/782] Loss_D: 0.7625 Loss_G: 7.5430\n",
      "[0/25][117/782] Loss_D: 0.6735 Loss_G: 5.0935\n",
      "[0/25][118/782] Loss_D: 0.1784 Loss_G: 3.6587\n",
      "[0/25][119/782] Loss_D: 0.5066 Loss_G: 7.0248\n",
      "[0/25][120/782] Loss_D: 0.3770 Loss_G: 5.6616\n",
      "[0/25][121/782] Loss_D: 0.2980 Loss_G: 3.2689\n",
      "[0/25][122/782] Loss_D: 0.4978 Loss_G: 5.8030\n",
      "[0/25][123/782] Loss_D: 0.2771 Loss_G: 5.0141\n",
      "[0/25][124/782] Loss_D: 0.3665 Loss_G: 3.3089\n",
      "[0/25][125/782] Loss_D: 0.6368 Loss_G: 6.1621\n",
      "[0/25][126/782] Loss_D: 0.4813 Loss_G: 4.0538\n",
      "[0/25][127/782] Loss_D: 0.5269 Loss_G: 4.3114\n",
      "[0/25][128/782] Loss_D: 0.5301 Loss_G: 4.9799\n",
      "[0/25][129/782] Loss_D: 0.4142 Loss_G: 5.2305\n",
      "[0/25][130/782] Loss_D: 0.5891 Loss_G: 3.1611\n",
      "[0/25][131/782] Loss_D: 1.0335 Loss_G: 12.1011\n",
      "[0/25][132/782] Loss_D: 4.3523 Loss_G: 6.0522\n",
      "[0/25][133/782] Loss_D: 0.2512 Loss_G: 2.4663\n",
      "[0/25][134/782] Loss_D: 1.2201 Loss_G: 7.9999\n",
      "[0/25][135/782] Loss_D: 1.4162 Loss_G: 4.9290\n",
      "[0/25][136/782] Loss_D: 0.2861 Loss_G: 2.7644\n",
      "[0/25][137/782] Loss_D: 1.3012 Loss_G: 9.8472\n",
      "[0/25][138/782] Loss_D: 1.2706 Loss_G: 7.6781\n",
      "[0/25][139/782] Loss_D: 0.5432 Loss_G: 3.8497\n",
      "[0/25][140/782] Loss_D: 1.1218 Loss_G: 5.4312\n",
      "[0/25][141/782] Loss_D: 0.4116 Loss_G: 5.3370\n",
      "[0/25][142/782] Loss_D: 0.3562 Loss_G: 4.1484\n",
      "[0/25][143/782] Loss_D: 0.3177 Loss_G: 4.0944\n",
      "[0/25][144/782] Loss_D: 0.4347 Loss_G: 3.9860\n",
      "[0/25][145/782] Loss_D: 0.3078 Loss_G: 4.3293\n",
      "[0/25][146/782] Loss_D: 0.4103 Loss_G: 3.5446\n",
      "[0/25][147/782] Loss_D: 0.5604 Loss_G: 4.2790\n",
      "[0/25][148/782] Loss_D: 0.3754 Loss_G: 4.7484\n",
      "[0/25][149/782] Loss_D: 0.6167 Loss_G: 2.7215\n",
      "[0/25][150/782] Loss_D: 0.8241 Loss_G: 9.3249\n",
      "[0/25][151/782] Loss_D: 1.6564 Loss_G: 5.4138\n",
      "[0/25][152/782] Loss_D: 0.3053 Loss_G: 2.5316\n",
      "[0/25][153/782] Loss_D: 1.1530 Loss_G: 7.3872\n",
      "[0/25][154/782] Loss_D: 1.1006 Loss_G: 4.8554\n",
      "[0/25][155/782] Loss_D: 0.3045 Loss_G: 3.6833\n",
      "[0/25][156/782] Loss_D: 0.7684 Loss_G: 5.3482\n",
      "[0/25][157/782] Loss_D: 0.4624 Loss_G: 4.2600\n",
      "[0/25][158/782] Loss_D: 0.3948 Loss_G: 4.8787\n",
      "[0/25][159/782] Loss_D: 0.4064 Loss_G: 3.6621\n",
      "[0/25][160/782] Loss_D: 0.5469 Loss_G: 7.0398\n",
      "[0/25][161/782] Loss_D: 0.3407 Loss_G: 5.5180\n",
      "[0/25][162/782] Loss_D: 0.3081 Loss_G: 3.1829\n",
      "[0/25][163/782] Loss_D: 0.5275 Loss_G: 5.6668\n",
      "[0/25][164/782] Loss_D: 0.4314 Loss_G: 4.6800\n",
      "[0/25][165/782] Loss_D: 0.3213 Loss_G: 4.5489\n",
      "[0/25][166/782] Loss_D: 0.4755 Loss_G: 4.7506\n",
      "[0/25][167/782] Loss_D: 0.2988 Loss_G: 5.0464\n",
      "[0/25][168/782] Loss_D: 0.2423 Loss_G: 5.3018\n",
      "[0/25][169/782] Loss_D: 0.2578 Loss_G: 5.4657\n",
      "[0/25][170/782] Loss_D: 0.2882 Loss_G: 4.6301\n",
      "[0/25][171/782] Loss_D: 0.4731 Loss_G: 8.5889\n",
      "[0/25][172/782] Loss_D: 0.3768 Loss_G: 5.7276\n",
      "[0/25][173/782] Loss_D: 0.2847 Loss_G: 5.6767\n",
      "[0/25][174/782] Loss_D: 0.2733 Loss_G: 6.4518\n",
      "[0/25][175/782] Loss_D: 0.3585 Loss_G: 4.8222\n",
      "[0/25][176/782] Loss_D: 0.4697 Loss_G: 7.2463\n",
      "[0/25][177/782] Loss_D: 0.2932 Loss_G: 5.4602\n",
      "[0/25][178/782] Loss_D: 0.2156 Loss_G: 5.2002\n",
      "[0/25][179/782] Loss_D: 0.4163 Loss_G: 8.0572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][180/782] Loss_D: 0.6356 Loss_G: 3.9650\n",
      "[0/25][181/782] Loss_D: 0.4209 Loss_G: 7.7804\n",
      "[0/25][182/782] Loss_D: 0.0853 Loss_G: 7.4761\n",
      "[0/25][183/782] Loss_D: 0.1713 Loss_G: 5.3782\n",
      "[0/25][184/782] Loss_D: 0.2322 Loss_G: 6.0610\n",
      "[0/25][185/782] Loss_D: 0.1860 Loss_G: 6.1565\n",
      "[0/25][186/782] Loss_D: 0.2238 Loss_G: 4.8479\n",
      "[0/25][187/782] Loss_D: 0.2833 Loss_G: 5.9690\n",
      "[0/25][188/782] Loss_D: 0.3604 Loss_G: 5.6354\n",
      "[0/25][189/782] Loss_D: 0.2149 Loss_G: 4.9929\n",
      "[0/25][190/782] Loss_D: 0.1741 Loss_G: 6.5946\n",
      "[0/25][191/782] Loss_D: 0.1385 Loss_G: 5.9625\n",
      "[0/25][192/782] Loss_D: 0.1774 Loss_G: 5.3845\n",
      "[0/25][193/782] Loss_D: 0.1705 Loss_G: 6.2054\n",
      "[0/25][194/782] Loss_D: 0.2273 Loss_G: 5.0159\n",
      "[0/25][195/782] Loss_D: 0.1667 Loss_G: 5.8277\n",
      "[0/25][196/782] Loss_D: 0.1579 Loss_G: 5.4027\n",
      "[0/25][197/782] Loss_D: 0.1173 Loss_G: 5.9153\n",
      "[0/25][198/782] Loss_D: 0.3730 Loss_G: 3.3799\n",
      "[0/25][199/782] Loss_D: 0.7212 Loss_G: 13.0564\n",
      "[0/25][200/782] Loss_D: 0.4221 Loss_G: 13.4035\n",
      "[0/25][201/782] Loss_D: 0.1768 Loss_G: 8.4368\n",
      "[0/25][202/782] Loss_D: 0.1321 Loss_G: 6.0878\n",
      "[0/25][203/782] Loss_D: 0.9298 Loss_G: 17.7816\n",
      "[0/25][204/782] Loss_D: 1.7009 Loss_G: 13.6542\n",
      "[0/25][205/782] Loss_D: 0.1825 Loss_G: 8.2546\n",
      "[0/25][206/782] Loss_D: 0.1865 Loss_G: 5.8789\n",
      "[0/25][207/782] Loss_D: 0.4497 Loss_G: 10.1092\n",
      "[0/25][208/782] Loss_D: 0.9971 Loss_G: 3.5183\n",
      "[0/25][209/782] Loss_D: 0.3720 Loss_G: 6.9439\n",
      "[0/25][210/782] Loss_D: 0.0653 Loss_G: 7.3356\n",
      "[0/25][211/782] Loss_D: 0.0759 Loss_G: 6.3918\n",
      "[0/25][212/782] Loss_D: 0.1460 Loss_G: 5.1430\n",
      "[0/25][213/782] Loss_D: 0.2343 Loss_G: 4.8118\n",
      "[0/25][214/782] Loss_D: 0.2244 Loss_G: 6.4544\n",
      "[0/25][215/782] Loss_D: 0.1959 Loss_G: 5.3374\n",
      "[0/25][216/782] Loss_D: 0.2380 Loss_G: 6.0929\n",
      "[0/25][217/782] Loss_D: 0.3509 Loss_G: 4.6404\n",
      "[0/25][218/782] Loss_D: 0.3284 Loss_G: 6.8428\n",
      "[0/25][219/782] Loss_D: 0.1614 Loss_G: 6.6600\n",
      "[0/25][220/782] Loss_D: 0.1633 Loss_G: 5.0738\n",
      "[0/25][221/782] Loss_D: 0.1917 Loss_G: 6.5403\n",
      "[0/25][222/782] Loss_D: 0.0871 Loss_G: 6.3597\n",
      "[0/25][223/782] Loss_D: 0.1766 Loss_G: 4.7594\n",
      "[0/25][224/782] Loss_D: 0.1927 Loss_G: 7.3068\n",
      "[0/25][225/782] Loss_D: 0.0753 Loss_G: 6.7564\n",
      "[0/25][226/782] Loss_D: 0.2021 Loss_G: 4.9528\n",
      "[0/25][227/782] Loss_D: 0.3553 Loss_G: 7.7178\n",
      "[0/25][228/782] Loss_D: 0.1624 Loss_G: 6.6951\n",
      "[0/25][229/782] Loss_D: 0.1801 Loss_G: 4.8765\n",
      "[0/25][230/782] Loss_D: 0.4709 Loss_G: 9.7274\n",
      "[0/25][231/782] Loss_D: 0.1863 Loss_G: 9.3235\n",
      "[0/25][232/782] Loss_D: 0.1448 Loss_G: 6.3697\n",
      "[0/25][233/782] Loss_D: 0.2115 Loss_G: 7.9419\n",
      "[0/25][234/782] Loss_D: 0.0853 Loss_G: 7.5617\n",
      "[0/25][235/782] Loss_D: 0.1622 Loss_G: 6.5408\n",
      "[0/25][236/782] Loss_D: 0.1812 Loss_G: 7.2692\n",
      "[0/25][237/782] Loss_D: 0.2161 Loss_G: 5.6934\n",
      "[0/25][238/782] Loss_D: 0.3948 Loss_G: 5.9798\n",
      "[0/25][239/782] Loss_D: 0.2495 Loss_G: 9.0228\n",
      "[0/25][240/782] Loss_D: 0.2452 Loss_G: 7.1175\n",
      "[0/25][241/782] Loss_D: 0.1562 Loss_G: 4.8291\n",
      "[0/25][242/782] Loss_D: 0.4422 Loss_G: 9.2826\n",
      "[0/25][243/782] Loss_D: 0.5236 Loss_G: 6.2013\n",
      "[0/25][244/782] Loss_D: 0.0993 Loss_G: 4.4579\n",
      "[0/25][245/782] Loss_D: 0.4136 Loss_G: 10.0764\n",
      "[0/25][246/782] Loss_D: 0.1769 Loss_G: 9.7445\n",
      "[0/25][247/782] Loss_D: 0.2212 Loss_G: 6.8461\n",
      "[0/25][248/782] Loss_D: 0.1663 Loss_G: 3.9726\n",
      "[0/25][249/782] Loss_D: 0.6823 Loss_G: 12.1771\n",
      "[0/25][250/782] Loss_D: 1.1152 Loss_G: 8.2212\n",
      "[0/25][251/782] Loss_D: 0.0523 Loss_G: 5.3947\n",
      "[0/25][252/782] Loss_D: 0.4527 Loss_G: 8.0300\n",
      "[0/25][253/782] Loss_D: 0.1125 Loss_G: 7.4200\n",
      "[0/25][254/782] Loss_D: 0.3966 Loss_G: 6.7965\n",
      "[0/25][255/782] Loss_D: 0.2873 Loss_G: 5.9211\n",
      "[0/25][256/782] Loss_D: 0.3678 Loss_G: 11.1701\n",
      "[0/25][257/782] Loss_D: 0.6527 Loss_G: 6.5338\n",
      "[0/25][258/782] Loss_D: 0.1701 Loss_G: 6.3385\n",
      "[0/25][259/782] Loss_D: 0.1913 Loss_G: 8.7567\n",
      "[0/25][260/782] Loss_D: 0.2024 Loss_G: 7.1116\n",
      "[0/25][261/782] Loss_D: 0.1879 Loss_G: 4.7175\n",
      "[0/25][262/782] Loss_D: 0.6147 Loss_G: 13.5434\n",
      "[0/25][263/782] Loss_D: 1.5039 Loss_G: 5.9992\n",
      "[0/25][264/782] Loss_D: 0.7366 Loss_G: 7.7217\n",
      "[0/25][265/782] Loss_D: 0.4566 Loss_G: 5.6352\n",
      "[0/25][266/782] Loss_D: 0.5613 Loss_G: 8.0119\n",
      "[0/25][267/782] Loss_D: 1.0514 Loss_G: 1.9749\n",
      "[0/25][268/782] Loss_D: 2.3160 Loss_G: 13.1732\n",
      "[0/25][269/782] Loss_D: 5.4006 Loss_G: 8.1714\n",
      "[0/25][270/782] Loss_D: 0.3539 Loss_G: 3.2116\n",
      "[0/25][271/782] Loss_D: 0.6179 Loss_G: 3.9887\n",
      "[0/25][272/782] Loss_D: 0.2722 Loss_G: 4.6203\n",
      "[0/25][273/782] Loss_D: 0.3829 Loss_G: 3.4397\n",
      "[0/25][274/782] Loss_D: 0.4821 Loss_G: 3.9745\n",
      "[0/25][275/782] Loss_D: 0.4115 Loss_G: 4.0282\n",
      "[0/25][276/782] Loss_D: 0.3938 Loss_G: 3.6159\n",
      "[0/25][277/782] Loss_D: 0.3466 Loss_G: 4.2095\n",
      "[0/25][278/782] Loss_D: 0.2294 Loss_G: 4.4139\n",
      "[0/25][279/782] Loss_D: 0.3743 Loss_G: 3.2954\n",
      "[0/25][280/782] Loss_D: 0.4170 Loss_G: 5.2964\n",
      "[0/25][281/782] Loss_D: 0.4656 Loss_G: 2.9465\n",
      "[0/25][282/782] Loss_D: 0.4608 Loss_G: 3.8727\n",
      "[0/25][283/782] Loss_D: 0.2079 Loss_G: 4.8956\n",
      "[0/25][284/782] Loss_D: 0.1845 Loss_G: 4.2212\n",
      "[0/25][285/782] Loss_D: 0.2663 Loss_G: 3.8154\n",
      "[0/25][286/782] Loss_D: 0.2607 Loss_G: 4.6069\n",
      "[0/25][287/782] Loss_D: 0.3504 Loss_G: 3.7397\n",
      "[0/25][288/782] Loss_D: 0.4944 Loss_G: 3.0822\n",
      "[0/25][289/782] Loss_D: 0.5133 Loss_G: 5.4293\n",
      "[0/25][290/782] Loss_D: 0.5024 Loss_G: 2.9060\n",
      "[0/25][291/782] Loss_D: 0.7555 Loss_G: 7.2753\n",
      "[0/25][292/782] Loss_D: 1.8589 Loss_G: 1.4287\n",
      "[0/25][293/782] Loss_D: 1.7746 Loss_G: 7.4365\n",
      "[0/25][294/782] Loss_D: 1.0361 Loss_G: 4.8874\n",
      "[0/25][295/782] Loss_D: 0.4731 Loss_G: 2.5311\n",
      "[0/25][296/782] Loss_D: 0.8186 Loss_G: 4.5305\n",
      "[0/25][297/782] Loss_D: 0.4841 Loss_G: 3.6638\n",
      "[0/25][298/782] Loss_D: 0.4377 Loss_G: 3.6255\n",
      "[0/25][299/782] Loss_D: 0.4585 Loss_G: 3.3859\n",
      "[0/25][300/782] Loss_D: 0.3322 Loss_G: 3.6120\n",
      "[0/25][301/782] Loss_D: 0.3040 Loss_G: 4.5542\n",
      "[0/25][302/782] Loss_D: 0.3019 Loss_G: 3.5841\n",
      "[0/25][303/782] Loss_D: 0.3132 Loss_G: 3.1790\n",
      "[0/25][304/782] Loss_D: 0.4862 Loss_G: 5.0951\n",
      "[0/25][305/782] Loss_D: 0.5801 Loss_G: 2.7287\n",
      "[0/25][306/782] Loss_D: 0.4423 Loss_G: 2.5752\n",
      "[0/25][307/782] Loss_D: 0.7061 Loss_G: 5.6715\n",
      "[0/25][308/782] Loss_D: 1.3849 Loss_G: 1.9317\n",
      "[0/25][309/782] Loss_D: 1.0978 Loss_G: 4.1010\n",
      "[0/25][310/782] Loss_D: 1.0289 Loss_G: 2.4387\n",
      "[0/25][311/782] Loss_D: 0.7103 Loss_G: 2.5830\n",
      "[0/25][312/782] Loss_D: 1.0077 Loss_G: 5.6238\n",
      "[0/25][313/782] Loss_D: 0.8633 Loss_G: 2.3816\n",
      "[0/25][314/782] Loss_D: 0.8076 Loss_G: 5.5924\n",
      "[0/25][315/782] Loss_D: 0.9592 Loss_G: 2.6922\n",
      "[0/25][316/782] Loss_D: 1.0650 Loss_G: 7.1879\n",
      "[0/25][317/782] Loss_D: 1.3709 Loss_G: 2.8035\n",
      "[0/25][318/782] Loss_D: 0.6075 Loss_G: 4.3692\n",
      "[0/25][319/782] Loss_D: 0.8430 Loss_G: 2.8914\n",
      "[0/25][320/782] Loss_D: 0.5183 Loss_G: 3.6701\n",
      "[0/25][321/782] Loss_D: 0.6545 Loss_G: 2.5448\n",
      "[0/25][322/782] Loss_D: 0.9272 Loss_G: 6.3085\n",
      "[0/25][323/782] Loss_D: 1.8248 Loss_G: 1.2566\n",
      "[0/25][324/782] Loss_D: 0.9745 Loss_G: 4.9698\n",
      "[0/25][325/782] Loss_D: 0.3707 Loss_G: 4.4331\n",
      "[0/25][326/782] Loss_D: 0.4854 Loss_G: 2.4823\n",
      "[0/25][327/782] Loss_D: 0.5443 Loss_G: 4.5564\n",
      "[0/25][328/782] Loss_D: 0.5860 Loss_G: 2.9405\n",
      "[0/25][329/782] Loss_D: 0.8009 Loss_G: 4.5665\n",
      "[0/25][330/782] Loss_D: 0.9551 Loss_G: 1.6886\n",
      "[0/25][331/782] Loss_D: 1.0655 Loss_G: 5.3163\n",
      "[0/25][332/782] Loss_D: 1.2231 Loss_G: 2.4624\n",
      "[0/25][333/782] Loss_D: 0.7569 Loss_G: 2.2728\n",
      "[0/25][334/782] Loss_D: 0.9466 Loss_G: 4.5083\n",
      "[0/25][335/782] Loss_D: 0.4397 Loss_G: 3.7304\n",
      "[0/25][336/782] Loss_D: 0.5855 Loss_G: 4.4330\n",
      "[0/25][337/782] Loss_D: 0.6665 Loss_G: 2.4565\n",
      "[0/25][338/782] Loss_D: 0.9119 Loss_G: 7.0538\n",
      "[0/25][339/782] Loss_D: 1.5210 Loss_G: 2.8616\n",
      "[0/25][340/782] Loss_D: 0.6058 Loss_G: 4.3173\n",
      "[0/25][341/782] Loss_D: 0.4808 Loss_G: 3.4331\n",
      "[0/25][342/782] Loss_D: 0.8261 Loss_G: 3.9220\n",
      "[0/25][343/782] Loss_D: 1.0035 Loss_G: 2.5936\n",
      "[0/25][344/782] Loss_D: 0.9539 Loss_G: 6.9257\n",
      "[0/25][345/782] Loss_D: 1.3194 Loss_G: 2.9194\n",
      "[0/25][346/782] Loss_D: 0.4717 Loss_G: 2.9450\n",
      "[0/25][347/782] Loss_D: 1.0803 Loss_G: 7.2956\n",
      "[0/25][348/782] Loss_D: 1.1540 Loss_G: 2.9266\n",
      "[0/25][349/782] Loss_D: 0.5387 Loss_G: 4.4160\n",
      "[0/25][350/782] Loss_D: 0.4127 Loss_G: 3.7833\n",
      "[0/25][351/782] Loss_D: 0.7240 Loss_G: 1.8171\n",
      "[0/25][352/782] Loss_D: 0.8366 Loss_G: 6.0783\n",
      "[0/25][353/782] Loss_D: 0.8491 Loss_G: 2.7594\n",
      "[0/25][354/782] Loss_D: 0.7680 Loss_G: 3.5271\n",
      "[0/25][355/782] Loss_D: 0.6053 Loss_G: 5.1080\n",
      "[0/25][356/782] Loss_D: 1.0469 Loss_G: 1.9629\n",
      "[0/25][357/782] Loss_D: 1.1542 Loss_G: 6.0754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][358/782] Loss_D: 0.5443 Loss_G: 4.2787\n",
      "[0/25][359/782] Loss_D: 0.5009 Loss_G: 4.4525\n",
      "[0/25][360/782] Loss_D: 0.3814 Loss_G: 3.5443\n",
      "[0/25][361/782] Loss_D: 0.4181 Loss_G: 3.8727\n",
      "[0/25][362/782] Loss_D: 0.5967 Loss_G: 3.3447\n",
      "[0/25][363/782] Loss_D: 0.4635 Loss_G: 4.5098\n",
      "[0/25][364/782] Loss_D: 0.4514 Loss_G: 2.9265\n",
      "[0/25][365/782] Loss_D: 0.6495 Loss_G: 3.1787\n",
      "[0/25][366/782] Loss_D: 0.7026 Loss_G: 5.7500\n",
      "[0/25][367/782] Loss_D: 1.0087 Loss_G: 1.2922\n",
      "[0/25][368/782] Loss_D: 1.2531 Loss_G: 6.8810\n",
      "[0/25][369/782] Loss_D: 2.0431 Loss_G: 1.1995\n",
      "[0/25][370/782] Loss_D: 1.6348 Loss_G: 7.2373\n",
      "[0/25][371/782] Loss_D: 1.7596 Loss_G: 2.9046\n",
      "[0/25][372/782] Loss_D: 0.8533 Loss_G: 3.7891\n",
      "[0/25][373/782] Loss_D: 0.7132 Loss_G: 4.3623\n",
      "[0/25][374/782] Loss_D: 1.2245 Loss_G: 1.4532\n",
      "[0/25][375/782] Loss_D: 1.6104 Loss_G: 5.6113\n",
      "[0/25][376/782] Loss_D: 1.5382 Loss_G: 2.3985\n",
      "[0/25][377/782] Loss_D: 0.5029 Loss_G: 3.4303\n",
      "[0/25][378/782] Loss_D: 0.4816 Loss_G: 4.3919\n",
      "[0/25][379/782] Loss_D: 0.9291 Loss_G: 1.5144\n",
      "[0/25][380/782] Loss_D: 0.8327 Loss_G: 4.8325\n",
      "[0/25][381/782] Loss_D: 0.4966 Loss_G: 3.1558\n",
      "[0/25][382/782] Loss_D: 0.3307 Loss_G: 2.7311\n",
      "[0/25][383/782] Loss_D: 0.5742 Loss_G: 3.3435\n",
      "[0/25][384/782] Loss_D: 0.4087 Loss_G: 3.2325\n",
      "[0/25][385/782] Loss_D: 0.3131 Loss_G: 3.4048\n",
      "[0/25][386/782] Loss_D: 0.2933 Loss_G: 4.0000\n",
      "[0/25][387/782] Loss_D: 0.3814 Loss_G: 2.9806\n",
      "[0/25][388/782] Loss_D: 0.3480 Loss_G: 4.0577\n",
      "[0/25][389/782] Loss_D: 0.4660 Loss_G: 2.5400\n",
      "[0/25][390/782] Loss_D: 0.4132 Loss_G: 3.0524\n",
      "[0/25][391/782] Loss_D: 0.5188 Loss_G: 4.3536\n",
      "[0/25][392/782] Loss_D: 0.3541 Loss_G: 3.5235\n",
      "[0/25][393/782] Loss_D: 0.4109 Loss_G: 2.9835\n",
      "[0/25][394/782] Loss_D: 0.9270 Loss_G: 4.7552\n",
      "[0/25][395/782] Loss_D: 1.1378 Loss_G: 0.2136\n",
      "[0/25][396/782] Loss_D: 2.9111 Loss_G: 6.9151\n",
      "[0/25][397/782] Loss_D: 1.6102 Loss_G: 4.0660\n",
      "[0/25][398/782] Loss_D: 0.6519 Loss_G: 0.9866\n",
      "[0/25][399/782] Loss_D: 0.8624 Loss_G: 2.9662\n",
      "[0/25][400/782] Loss_D: 0.4756 Loss_G: 4.4625\n",
      "[0/25][401/782] Loss_D: 0.4703 Loss_G: 3.7597\n",
      "[0/25][402/782] Loss_D: 0.6866 Loss_G: 1.9082\n",
      "[0/25][403/782] Loss_D: 0.8069 Loss_G: 4.2952\n",
      "[0/25][404/782] Loss_D: 0.3710 Loss_G: 3.5325\n",
      "[0/25][405/782] Loss_D: 0.4383 Loss_G: 2.0693\n",
      "[0/25][406/782] Loss_D: 0.9824 Loss_G: 5.2087\n",
      "[0/25][407/782] Loss_D: 0.7900 Loss_G: 1.0384\n",
      "[0/25][408/782] Loss_D: 1.3995 Loss_G: 7.7790\n",
      "[0/25][409/782] Loss_D: 2.2198 Loss_G: 1.4061\n",
      "[0/25][410/782] Loss_D: 0.7685 Loss_G: 1.9080\n",
      "[0/25][411/782] Loss_D: 0.8094 Loss_G: 4.5067\n",
      "[0/25][412/782] Loss_D: 1.1337 Loss_G: 1.3485\n",
      "[0/25][413/782] Loss_D: 0.7787 Loss_G: 3.4704\n",
      "[0/25][414/782] Loss_D: 0.9071 Loss_G: 1.7783\n",
      "[0/25][415/782] Loss_D: 0.7910 Loss_G: 4.8431\n",
      "[0/25][416/782] Loss_D: 0.8735 Loss_G: 2.1917\n",
      "[0/25][417/782] Loss_D: 0.4690 Loss_G: 2.3958\n",
      "[0/25][418/782] Loss_D: 0.2902 Loss_G: 3.8031\n",
      "[0/25][419/782] Loss_D: 0.3439 Loss_G: 3.2867\n",
      "[0/25][420/782] Loss_D: 0.3614 Loss_G: 2.7928\n",
      "[0/25][421/782] Loss_D: 0.2445 Loss_G: 3.7098\n",
      "[0/25][422/782] Loss_D: 0.2428 Loss_G: 3.6640\n",
      "[0/25][423/782] Loss_D: 0.3060 Loss_G: 2.9480\n",
      "[0/25][424/782] Loss_D: 0.2973 Loss_G: 3.7197\n",
      "[0/25][425/782] Loss_D: 0.1926 Loss_G: 3.9724\n",
      "[0/25][426/782] Loss_D: 0.1872 Loss_G: 3.7789\n",
      "[0/25][427/782] Loss_D: 0.3290 Loss_G: 3.1419\n",
      "[0/25][428/782] Loss_D: 0.2806 Loss_G: 3.6477\n",
      "[0/25][429/782] Loss_D: 0.2021 Loss_G: 4.0709\n",
      "[0/25][430/782] Loss_D: 0.2427 Loss_G: 4.1668\n",
      "[0/25][431/782] Loss_D: 0.4172 Loss_G: 2.6230\n",
      "[0/25][432/782] Loss_D: 0.5209 Loss_G: 6.0796\n",
      "[0/25][433/782] Loss_D: 0.2160 Loss_G: 6.0167\n",
      "[0/25][434/782] Loss_D: 0.1598 Loss_G: 4.2795\n",
      "[0/25][435/782] Loss_D: 0.4082 Loss_G: 5.5432\n",
      "[0/25][436/782] Loss_D: 0.2461 Loss_G: 5.8626\n",
      "[0/25][437/782] Loss_D: 0.4484 Loss_G: 4.0092\n",
      "[0/25][438/782] Loss_D: 0.9077 Loss_G: 7.6674\n",
      "[0/25][439/782] Loss_D: 0.1995 Loss_G: 5.4810\n",
      "[0/25][440/782] Loss_D: 0.6135 Loss_G: 3.4236\n",
      "[0/25][441/782] Loss_D: 2.0615 Loss_G: 6.8860\n",
      "[0/25][442/782] Loss_D: 2.5288 Loss_G: 2.5373\n",
      "[0/25][443/782] Loss_D: 0.7869 Loss_G: 2.6233\n",
      "[0/25][444/782] Loss_D: 0.9806 Loss_G: 5.3947\n",
      "[0/25][445/782] Loss_D: 0.9909 Loss_G: 3.6387\n",
      "[0/25][446/782] Loss_D: 0.6526 Loss_G: 4.9912\n",
      "[0/25][447/782] Loss_D: 0.3846 Loss_G: 4.3121\n",
      "[0/25][448/782] Loss_D: 0.3548 Loss_G: 3.3351\n",
      "[0/25][449/782] Loss_D: 0.6825 Loss_G: 5.7809\n",
      "[0/25][450/782] Loss_D: 0.9162 Loss_G: 1.5651\n",
      "[0/25][451/782] Loss_D: 1.5008 Loss_G: 7.7197\n",
      "[0/25][452/782] Loss_D: 1.0276 Loss_G: 4.0596\n",
      "[0/25][453/782] Loss_D: 0.4925 Loss_G: 1.2060\n",
      "[0/25][454/782] Loss_D: 1.6430 Loss_G: 8.5181\n",
      "[0/25][455/782] Loss_D: 1.7778 Loss_G: 5.0720\n",
      "[0/25][456/782] Loss_D: 0.3813 Loss_G: 1.8936\n",
      "[0/25][457/782] Loss_D: 0.7539 Loss_G: 4.0052\n",
      "[0/25][458/782] Loss_D: 0.4374 Loss_G: 3.9507\n",
      "[0/25][459/782] Loss_D: 0.4647 Loss_G: 2.7848\n",
      "[0/25][460/782] Loss_D: 0.4809 Loss_G: 3.1993\n",
      "[0/25][461/782] Loss_D: 0.3826 Loss_G: 4.0798\n",
      "[0/25][462/782] Loss_D: 0.3259 Loss_G: 3.7369\n",
      "[0/25][463/782] Loss_D: 0.3173 Loss_G: 3.8179\n",
      "[0/25][464/782] Loss_D: 0.3317 Loss_G: 4.0476\n",
      "[0/25][465/782] Loss_D: 0.3950 Loss_G: 3.2475\n",
      "[0/25][466/782] Loss_D: 0.4279 Loss_G: 4.8286\n",
      "[0/25][467/782] Loss_D: 0.3966 Loss_G: 3.2083\n",
      "[0/25][468/782] Loss_D: 0.2778 Loss_G: 4.0143\n",
      "[0/25][469/782] Loss_D: 0.2243 Loss_G: 4.6581\n",
      "[0/25][470/782] Loss_D: 0.1647 Loss_G: 4.8726\n",
      "[0/25][471/782] Loss_D: 0.2911 Loss_G: 3.3293\n",
      "[0/25][472/782] Loss_D: 0.2869 Loss_G: 4.1926\n",
      "[0/25][473/782] Loss_D: 0.5676 Loss_G: 3.4454\n",
      "[0/25][474/782] Loss_D: 0.7498 Loss_G: 5.5209\n",
      "[0/25][475/782] Loss_D: 0.8614 Loss_G: 1.8161\n",
      "[0/25][476/782] Loss_D: 0.6845 Loss_G: 6.2467\n",
      "[0/25][477/782] Loss_D: 0.4904 Loss_G: 4.5904\n",
      "[0/25][478/782] Loss_D: 0.2073 Loss_G: 3.0087\n",
      "[0/25][479/782] Loss_D: 0.4442 Loss_G: 4.5909\n",
      "[0/25][480/782] Loss_D: 0.2779 Loss_G: 4.3012\n",
      "[0/25][481/782] Loss_D: 0.3692 Loss_G: 3.0195\n",
      "[0/25][482/782] Loss_D: 0.7225 Loss_G: 6.8617\n",
      "[0/25][483/782] Loss_D: 0.8911 Loss_G: 3.1643\n",
      "[0/25][484/782] Loss_D: 0.5910 Loss_G: 4.0810\n",
      "[0/25][485/782] Loss_D: 0.2675 Loss_G: 5.3823\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-47660be38a7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Training the discriminator with a fake image generated by the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mfake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b3cc648f5f83>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torchEnv\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torchEnv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    688\u001b[0m         return F.conv_transpose2d(\n\u001b[0;32m    689\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating the generator\n",
    "netG = G()\n",
    "netG.apply(weights_init)\n",
    "netD = D()\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Training the DCGANs\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "\n",
    "for epoch in range(25):\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data\n",
    "        input = Variable(real)\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(input)\n",
    "        errD_real = criterion(output, target)\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
    "        fake = netG(noise)\n",
    "        target = Variable(torch.zeros(input.size()[0]))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, target)\n",
    "        \n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG.zero_grad()\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0]))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
    "            fake = netG(noise)\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
